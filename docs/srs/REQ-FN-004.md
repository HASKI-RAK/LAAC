---
id: REQ-FN-004
title: Compute Analytics from xAPI LRS per CSV Metric
type: Functional
status: Draft
priority: High
stakeholder_trace: SG-4-003
owner: TODO
version: 0.1
---

## Description
The system shall compute analytics results for every metric defined in `docs/resources/LAAC_Learning_Analytics_Requirements.csv` using data retrieved from the configured xAPI LRS. For each metric, the system shall implement the required xAPI queries and aggregations to produce deterministic outputs.

## Rationale
Delivers the core value of the service: turning LRS statements into actionable analytics aligned with the CSV catalog.

## Acceptance Criteria
- For every CSV metric, a corresponding computation is implemented and reachable through the service API.
- Computations operate over xAPI data only (see REQ-NF-001) and support filtering by time range and, where applicable, by course, topic, learning element, and student.
- If a metric requires a notion of "best attempt", the selection criteria are defined and documented consistently across metrics.
- Missing data or gaps in xAPI statements result in well-defined empty/zero outputs rather than failures, with explanatory metadata.

## Verification
- Unit tests with seeded/mock xAPI statements validate the computations for representative metrics across the three dashboard levels (course, topic, element).
- E2E tests call the API endpoints for several metrics and verify expected values based on the mock dataset.

## Dependencies
- xAPI integration (REQ-FN-002).
- Catalog/discovery (REQ-FN-003).

## Assumptions / Constraints
- The CSV does not prescribe exact formulas; the initial definitions follow common-sense interpretations documented in code and API docs.
- Time range filters default to inclusive [start, end].

## API/Interface Impact
- Endpoint pattern: GET /metrics/{id}/results with query params for `actorId`, `courseId`, `topicId`, `elementId`, `start`, `end` as applicable per metric.
- Responses include: `metricId`, `value`, `unit` (if applicable), `scope` identifiers, and optional `explain` metadata.

## Observability
- Each computation logs duration and key filter dimensions; counters for success/failure per metric are exported.

## Risks / Open Questions
- Formal definition of "best attempt" may evolve; tracked as a versioned interpretation in docs.

## References
- Stakeholder Need(s): [SG-4-003](../strs-needs/SG-4-003.md)
- CSV: [LAAC_Learning_Analytics_Requirements.csv](../resources/LAAC_Learning_Analytics_Requirements.csv)

## Change History
- v0.1 â€” Initial draft

