# SG-4-004 â€” Developers

- Stakeholder Group: Developers
- Need: The system shall cache computed learning analytics so that metrics are not recomputed when the underlying LRS data and request parameters have not changed, reducing response latency for external clients.
- Rationale: Faster responses improve developer productivity and client UX; avoids unnecessary compute when results would be identical.
- Outcome: Repeat requests with unchanged inputs return a cached result instead of triggering full recomputation.
- Acceptance Criteria:
  - For identical request parameters and unchanged relevant LRS data, subsequent responses are served from cache (cache hit is observable in logs/metrics or via a test hook).
  - Cached responses are faster than a forced recomputation baseline under comparable conditions.
  - Cache invalidates when relevant LRS data changes (e.g., via xAPI timestamps/ETag/Last-Modified or a derived content hash) or when TTL expires.
  - Tests verify cache hit/miss behavior, invalidation on data change, and performance improvement vs. recomputation.
- Priority: Medium
- Assumptions: The LRS or ingestion layer exposes sufficient signals (timestamps/versions) to detect relevant data changes; cache keys include all request parameters that influence results (e.g., learner, course/topic, time range).

