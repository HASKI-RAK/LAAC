# SG-4-010 — Developers

- Stakeholder Group: Developers
- Need: Client requests to analytics endpoints shall complete within 1 second end-to-end under normal operating conditions.
- Rationale: Ensures responsive integrations and good UX; avoids client timeouts and enables interactive usage.
- Outcome: The service consistently meets a defined latency objective for read/analytics endpoints.
- Acceptance Criteria:
  - SLO: For read/analytics endpoints, p95 end-to-end latency ≤ 1s under documented nominal load profile and dataset size; p50 ≤ 500ms (target). Cache hits should typically be faster than misses.
  - Load profile: Expected peak load is 20 concurrent students with typical activity of 1-2 students in parallel. Dataset represents a handful of courses from multiple professors. System must handle this load without horizontal scaling.
  - Dataset size: Up to 20 active students × handful of courses (≤10 courses) × typical xAPI statement volume per student per course.
  - Cold start/first-run: Initial warm-up allowed; subsequent requests must meet SLO. Cold cache scenarios should complete within 2s p95.
  - Measurement: Latency histograms exported via metrics; e2e performance test can verify SLO in a controlled environment.
  - Degradations: When SLO cannot be met (e.g., external LRS slowness), system surfaces appropriate status/telemetry and avoids cascading failures (e.g., timeouts, retries with backoff).
- Priority: High
- Assumptions: Network to LRS is reliable with typical latency budget; heavy computations can leverage caching (see SG-4-004) or precomputation where needed; vertical scaling (CPU/memory) is available but no horizontal scaling required.

